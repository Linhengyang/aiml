{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1dcca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "from src.projs.transformer.dataset import *\n",
    "import os\n",
    "from src.core.utils.text.vocabulize import Vocab\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f0841",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# base_path = \"../../data\"\n",
    "# proj_folder = \"WMT14/en-fr\"\n",
    "# data_file = \"167130.txt\"\n",
    "\n",
    "# fname = os.path.join(base_path, proj_folder, data_file)\n",
    "# print(fname)\n",
    "\n",
    "# # test vocab\n",
    "\n",
    "# ## token 2D list\n",
    "# text = read_text2str((fname)) # raw text\n",
    "# text = preprocess_space(text, True, normalize_whitespace=False) # 保留 /t, 因为source和target是用 /t 分开的\n",
    "\n",
    "# source, target = tokenize_seq2seq(text, line_tokenize_simple, None, num_examples=10) # 使用 line_tokenize_simple 作 token化. 此时不需要 symbols\n",
    "\n",
    "# print(\"token 2d list\")\n",
    "# print(\"source\\n\", source)\n",
    "# print(\"target\\n\", target)\n",
    "\n",
    "# ## make vocab: \n",
    "# src_vocab = Vocab(source, min_freq = 0, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "# print(\"tokens' id as\")\n",
    "# print('<pad>', src_vocab['<pad>'])\n",
    "# print('<bos>', src_vocab['<bos>'])\n",
    "# print('<eos>', src_vocab['<eos>'])\n",
    "# print('go', src_vocab['go'])\n",
    "\n",
    "# print(\"ids' token as\")\n",
    "# print(\"1\", src_vocab.to_tokens(1))\n",
    "# print(\"0\", src_vocab.to_tokens(0))\n",
    "\n",
    "# ## vocab supports hierarchy of tokens\n",
    "# print(\"token sequence as\")\n",
    "# print(\"go run who ! . <unk> hi go . <pad> as \", src_vocab[[\"go\", \"run\", \"who\", \"!\", \".\", \"<unk>\", \"hi\", \"go\", \".\", \"<pad>\"]])\n",
    "\n",
    "\n",
    "# # test Tensor\n",
    "\n",
    "# ## id 映射\n",
    "# lines = [src_vocab[line] + [src_vocab['<eos>']] for line in source]\n",
    "# print(\"mapped tokens as\", source)\n",
    "# print(\"mapped token ids as\", lines)\n",
    "\n",
    "# ## truncate or pad: 截断/补齐 每一行 line 到 给定长度\n",
    "# num_steps = 5\n",
    "# aligned_lines = [truncate_pad(l, num_steps, src_vocab['<pad>']) for l in lines] # 2D list: shape as (num_lines, num_steps)\n",
    "# print(\"aligned_lines as\\n\", aligned_lines)\n",
    "\n",
    "# array = torch.tensor(aligned_lines)\n",
    "# print(\"array as\\n\", array)\n",
    "\n",
    "# ## valid lens: 需要保存 每一行 line 中 非 pad 的token 的数量信息: (num_lines, )\n",
    "# valid_lens = (array != src_vocab['<pad>']).type(torch.int32).sum(1)\n",
    "# print(\"valid lens(non-padding) as \\n\", valid_lens)\n",
    "\n",
    "\n",
    "# # test torch dataset\n",
    "# class testTorchDataset(torch.utils.data.Dataset):\n",
    "#     '''\n",
    "#     torch Dataset 要继承自 torch.utils.data.Dataset 类, 并提供三个 内置函数\n",
    "#     1、__init__方法: 读取并定义 ALL Data into tensor data 形式\n",
    "#     2、__getitem__方法: 有一个固定 param index, 以 index 定义 单个样本 single example of index 如何从该 Dataset 对象中释出\n",
    "#     3、__len__方法: 返回 ALL Data 的 data size\n",
    "    \n",
    "#     __init__方法确认了 ALL Data. __getitem__方法确认了 single datapoint. 它和__len__方法一起帮助 torch 的 dataiter工具从 ALL Data中生成 Data Batch\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, path, num_steps, num_examples=None):\n",
    "#         super().__init__() # torch dataset 继承 torch.utils.data.Dataset\n",
    "#         (X, X_valid_lens, Y, Y_valid_lens), (src_vocab, tgt_vocab) = build_dataset_vocab(path, num_steps, num_examples)\n",
    "#         # X, Y: (num_examples, num_steps) padding sequence data\n",
    "#         # X_valid_lens, Y_valid_lens: (num_examples,) valid length info data\n",
    "#         # src_vocab, tgt_vocab: mapping vocab for source and target text\n",
    "\n",
    "#         self._data_size = Y.shape[0]\n",
    "\n",
    "#         # bos: tensor with shape (num_examples, 1) to append before Y (num_examples, num_steps)\n",
    "#         bos = torch.tensor( [tgt_vocab['<bos>']] * self._data_size, device=Y.device ).reshape(-1, 1)\n",
    "#         # concatenate bos and Y's num_steps-1 on dim 1 to create dec_X (num_examples, num_steps)\n",
    "#         dec_X = torch.cat([bos, Y[:, :-1]], dim=1)\n",
    "\n",
    "#         self._net_inputs = (X, dec_X, X_valid_lens)\n",
    "#         self._loss_inputs = (Y, Y_valid_lens)\n",
    "\n",
    "#         self._src_vocab = src_vocab\n",
    "#         self._tgt_vocab = tgt_vocab\n",
    "    \n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return (tuple(tensor[index] for tensor in self._net_inputs),\n",
    "#                 tuple(tensor[index] for tensor in self._loss_inputs))\n",
    "\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self._data_size\n",
    "    \n",
    "#     @property\n",
    "#     def src_vocab(self):\n",
    "#         return self._src_vocab\n",
    "    \n",
    "#     @property\n",
    "#     def tgt_vocab(self):\n",
    "#         return self._tgt_vocab\n",
    "    \n",
    "    \n",
    "## torch dataset and data loader\n",
    "# trainset = testTorchDataset(fname, num_steps=5, num_examples=10)\n",
    "\n",
    "## print batch data\n",
    "# train_iter = torch.utils.data.DataLoader(trainset, batch_size=3, shuffle=True)\n",
    "# for x, y in train_iter:\n",
    "#     print(\"epoch 1\")\n",
    "#     print('x as\\n', x)\n",
    "#     print('y as\\n', y)\n",
    "\n",
    "# for x, y in train_iter:\n",
    "#     print(\"epoch 2\")\n",
    "#     print('x as\\n', x)\n",
    "#     print('y as\\n', y)\n",
    "\n",
    "## allocate batch data to another device\n",
    "\n",
    "## collate_fn param for DataLoader\n",
    "## Dataset对象中, __getitem__()返回的是 index 对应的单个 data point: data_ind, label_ind\n",
    "## Dataset对象传入 DataLoader对象中时, collate_fn 参数指定了一个 如何处理 batch_list 的函数, 以处理 batch_size 个 data point\n",
    "## collate_fn 默认是 default_collate 函数, 它将 batch_list(batch_size个data/label point组成的 list), 处理成shape为(batch_size,...)的DATA, \n",
    "## 和shape为(batch_size,...)的LABEL 两个 batch tensor dataset\n",
    "\n",
    "## collate_fn可以是自定义 batch 处理函数. 它应该满足: 以 batch_list(batch_size个__getitem__定义的datapoint组成的list)为输入, 输出batch处理后的结果\n",
    "# from torch.utils.data.dataloader import default_collate\n",
    "# device_cuda = torch.device('cuda')\n",
    "\n",
    "\n",
    "# def move_to_cuda(batch_list): \n",
    "#     '''\n",
    "#     __getitem__ 返回 datapoint of index:\n",
    "#         (tuple(tensor[index] for tensor in [X, dec_X, X_valid_lens]), tuple(tensor[index] for tensor in [Y, Y_valid_lens]))\n",
    "#         即:\n",
    "#         (X[index], dec_X[index], X_valid_lens[index]), (Y[index], Y_valid_lens[index])\n",
    "#     经过 default_collate(batch_list), 返回 batch data:\n",
    "#         (tuple(tensor[batch::] for tensor in [X, dec_X, X_valid_lens]), tuple(tensor[batch::] for tensor in [Y, Y_valid_lens]))\n",
    "#         即:\n",
    "#         (X[batch::], dec_X[batch::], X_valid_lens[batch::]), (Y[batch::], Y_valid_lens[batch::])\n",
    "#     逐一move到cuda上\n",
    "#     '''\n",
    "    \n",
    "#     # result = []\n",
    "#     # for x_ in default_collate(batch_list):\n",
    "#     #     res_ = tuple()\n",
    "#     #     for t in x_:\n",
    "#     #         res_ += (t.to(device_cuda),)\n",
    "#     #     result.append(res_)\n",
    "#     # return tuple(result)\n",
    "#     (X_batch, dec_X_batch, X_valid_lens_batch), (Y_batch, Y_valid_lens_batch) = default_collate(batch_list)\n",
    "\n",
    "#     X_batch = X_batch.to(device_cuda)\n",
    "#     dec_X_batch = dec_X_batch.to(device_cuda)\n",
    "#     X_valid_lens_batch = X_valid_lens_batch.to(device_cuda)\n",
    "#     Y_batch = Y_batch.to(device_cuda)\n",
    "#     Y_valid_lens_batch = Y_valid_lens_batch.to(device_cuda)\n",
    "\n",
    "#     return (X_batch, dec_X_batch, X_valid_lens_batch), (Y_batch, Y_valid_lens_batch)\n",
    "\n",
    "\n",
    "# train_iter = torch.utils.data.DataLoader(trainset, batch_size=5, shuffle=True, collate_fn=move_to_cuda)\n",
    "# for x, y in train_iter:\n",
    "#     print(\"epoch 3\")\n",
    "#     print('x in cuda\\n', x[0].device)\n",
    "#     print('y in cuda\\n', y[0].device)\n",
    "\n",
    "\n",
    "# network: transformer = encoder + decoder\n",
    "\n",
    "## encoder\n",
    "\n",
    "## mask_softmax\n",
    "## 制作一个 mask 以完成 mask_softmax 操作: 对于一个 (batch_size, n_queries, n_kvs) 的 score matrix batch, 总共有 batch_size*n_queries 条query\n",
    "## 对每条query 都只取 靠前的部分 score logits 参与softmax 计算。这个根据 不同 query 决定的 valid lens 储存在一个 (batch_size, n_queries) 的矩阵里 \n",
    "## 需要一个 mask tensor(batch_size, n_queries, n_kvs) , 其元素是True/False. 在 batch_i, n_queries_j 的位置, 前 valid_len 个 为True, 剩下的为False\n",
    "## batch_i, n_queries_j 的 valid_len，是由 valid_lens (batch_size, n_queries) 的(i,j)元 确定的\n",
    "## 原理就是 不断地让 [0, 1, ..., n_kvs-1] 和 valid_lens[i,j] 元素作 对比是否小于 的bool操作\n",
    "maxlen = 5\n",
    "\n",
    "## (-1, n_kvs) broadcast 机制\n",
    "query_indices = torch.arange(maxlen, dtype=torch.float32)\n",
    "print(query_indices)\n",
    "\n",
    "## (batch_size, n_queries)\n",
    "valid_lens = [[3, 4, 1, 0],\n",
    "                [2, 1, 0, 1],\n",
    "                [1, 4, 5, 5]]\n",
    "\n",
    "## (batch_size, n_queries, 1)\n",
    "valid_lens = torch.tensor(valid_lens).unsqueeze(2)\n",
    "print(valid_lens)\n",
    "\n",
    "## (batch_size, n_queries, n_kvs) broadcast机制\n",
    "mask = query_indices < valid_lens\n",
    "print( mask )\n",
    "\n",
    "## valid_lens 也可以是 (batch_size,) 的向量. 此时 对于 batch_i, 所有 query 的valid长度 都由 valid_lens[i] 决定\n",
    "## (batch_size, 1)\n",
    "valid_lens = torch.tensor([1, 3, 5]).unsqueeze(1)\n",
    "print(valid_lens)\n",
    "## (batch_size, n_queries)\n",
    "\n",
    "valid_lens = torch.repeat_interleave( valid_lens, repeats=4, dim=1)\n",
    "print(valid_lens)\n",
    "\n",
    "## index-put 操作 对 梯度反向传播的影响\n",
    "test_tensor = torch.tensor([1., 2., 3., 4.], requires_grad=True)\n",
    "\n",
    "# power 2 计算\n",
    "pow2_tensor = torch.pow(test_tensor, 2)\n",
    "\n",
    "# slice reset 计算\n",
    "pow2_tensor[2] = 1.\n",
    "\n",
    "y = torch.prod(pow2_tensor)\n",
    "y.backward()\n",
    "\n",
    "print('test_tensor.grad as ', test_tensor.grad)\n",
    "\n",
    "## 根据求梯度的链式法则, 被 slice-reset value 的变量, 由于被赋值了常数, 在梯度反传时它们不再贡献计算 梯度\n",
    "\n",
    "\n",
    "## 注意力机制 Atttention: Q K V --> sum of weights @ V, where weights = f(Q, K)\n",
    "## query/key/value 都有各自的数量和维度. 其中 query 的数量自由决定, 但是其维度要和key相同(毕竟要计算query和key之间的相似度)\n",
    "## value的维度自由决定, 但是其数量要和key相同(key决定了其对应value在最终输出结果中的重要程度)\n",
    "\n",
    "## ScaledDotProductAttention\n",
    "# 积式注意力 ScaledDotProductAttention 简单地根据 每条 query和不同keys之间地相似度, 决定了每个key对应的value的权重, 组合出最后的结果\n",
    "# 最终由 n_queries 条结果\n",
    "\n",
    "class dotProdAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q, K, V, valid_lens=None):\n",
    "        # Q(batch_size, n_query, qk_size), K(batch_size, n_kv, qk_size), V(batch_size, n_kv, v_size), \n",
    "        # valid_lens(batch_size, n_query) or (batch_size)\n",
    "\n",
    "        # Q K之间 相似度计算\n",
    "        d = Q.shape[2]\n",
    "        logits = torch.bmm(Q, K.permute(0, 2, 1)) / math.sqrt(d) # (batch_size, n_query, n_kv)\n",
    "\n",
    "        if valid_lens:\n",
    "            # 如果有 valid_lens, 需要用 mask 确保只有 valid logits 参与生成 概率分布\n",
    "            from src.core.base.functions.mask import mask_first_n_valid\n",
    "\n",
    "            mask = mask_first_n_valid(logits.shape, valid_lens)\n",
    "            logits[~mask] = -1e20 # invalid logits 用 负无穷 slice-reset. 此操作梯度可反传\n",
    "\n",
    "        # 如果没有 valid_lens, 直接使用 softmax 生成 概率分布 weights\n",
    "        weights = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        # 正则化 weights\n",
    "        return torch.bmm(self.dropout(weights), V)\n",
    "\n",
    "\n",
    "\n",
    "Q = torch.tensor([[3, 4, 1, 0],\n",
    "                    [2, 1, 0, 1],\n",
    "                    [1, 4, 5, 5]], dtype=torch.float32)\n",
    "Q = Q.unsqueeze(0).repeat(3, 1, 1)\n",
    "print(Q)\n",
    "\n",
    "net = dotProdAttention(0)\n",
    "\n",
    "y_hat = net(Q, Q, Q)\n",
    "print(y_hat)\n",
    "\n",
    "\n",
    "##\n",
    "## 多头注意力:\n",
    "##    单头注意力是指 对 QKV 作各自线性映射(至相同维度 num_hiddens/H )后, 作 ScaledDotProductAttention 后得到 (batch_size, n_queries, num_hiddens/H)\n",
    "## H 个这样的单头注意力的结果, 拼起来是一个 (batch_size, n_queries, num_hiddens) 的结果. 再follow一个 num_hiddens -> num_hiddens 的线性映射\n",
    "\n",
    "## 统一映射到 num_hiddens 维 -> Q(batch_size, n_query, num_hiddens), K(batch_size, n_kv, num_hiddens), V(batch_size, n_kv, num_hiddens)\n",
    "\n",
    "## 若可以切分成 H = num_heads 个头 reshape -> Q(batch_size, H, n_query, w), K(batch_size, H, n_kv, w), V(batch_size, H, n_kv, w)\n",
    "## 那么合并前两个维度, 即得 Q(batch_size*H, n_query, w), K(batch_size*H, n_kv, w), V(batch_size*H, n_kv, w)\n",
    "## 即可完成 DotProdAttention. \n",
    "\n",
    "## 从 (batch_size, n_, num_hiddens=H*w) 变换到 (batch_size, H, n_, w) 的方法如下:\n",
    "## (batch_size, n_, num_hiddens=H*w) --转置--> (batch_size, num_hiddens=H*w, n_) --> reshape--> (batch_size, H, w, n_) --转置-->  (batch_size, H, n_, w)\n",
    "## --reshape--> (batch_size*H, n_, w)\n",
    "\n",
    "def multihead_transpose_qkv(tensor, num_heads):\n",
    "\n",
    "    batch_size, n_, num_hiddens = tensor.shape # num_hiddens = num_heads * w\n",
    "    w = num_hiddens // num_heads\n",
    "\n",
    "    return tensor.permute(0, 2, 1).reshape(batch_size, num_heads, w, n_).permute(0, 1, 3, 2).reshape(-1, n_, w)\n",
    "\n",
    "## Q(batch_size*H, n_query, w) K(batch_size*H, n_kv, w)  V(batch_size*H, n_kv, w)  --dotProdAttention--> (batch_size*H, n_query, w)\n",
    "## 重建过程\n",
    "##  (batch_size*H, n_query, w) --reshape--> (batch_size, H, n_query, w) --转置--> (batch_size, H, w, n_query) --reshape--> (batch_size, H*w, n_query)\n",
    "## --转置--> (batch_size, n_query, num_hiddens=H*w)\n",
    "def multihead_transpose_o(tensor, num_heads):\n",
    "    _, n_query, w = tensor.shape # (batch_size*H, n_query, w)\n",
    "\n",
    "    return tensor.reshape(-1, num_heads, n_query, w).permute(0, 1, 3, 2).reshape(-1, num_heads*w, n_query).permute(0, 2, 1)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, num_hiddens, dropout, use_bias=False):\n",
    "        super().__init__()\n",
    "        self.H = num_heads\n",
    "        self.W_q = torch.nn.LazyLinear(num_hiddens, bias=use_bias)\n",
    "        self.W_k = torch.nn.LazyLinear(num_hiddens, bias=use_bias)\n",
    "        self.W_v = torch.nn.LazyLinear(num_hiddens, bias=use_bias)\n",
    "        self.attention = dotProdAttention(dropout)\n",
    "        self.W_o = torch.nn.LazyLinear(num_hiddens, bias=use_bias)\n",
    "\n",
    "    def forward(self, Q, K, V, valid_lens=None):\n",
    "        # Q(batch_size, n_query, qk_size), K(batch_size, n_kv, qk_size), V(batch_size, n_kv, v_size), \n",
    "        # valid_lens: (batch_size, n_query) / (batch_size,)  elements are integers <= n_kv\n",
    "\n",
    "        Q_ = multihead_transpose_qkv( self.W_q(Q), self.H ) # (batch_size*H, n_query, w). w = num_hiddens//H\n",
    "        K_ = multihead_transpose_qkv( self.W_k(K), self.H ) # (batch_size*H, n_kv, w). w = num_hiddens//H\n",
    "        # Q_ 和 K_ 确定的 weights: (batch_size*H, n_query, n_kv)\n",
    "        V_ = multihead_transpose_qkv( self.W_v(V), self.H ) # (batch_size*H, n_kv, w). w = num_hiddens//H\n",
    "\n",
    "        if valid_lens:\n",
    "            # valid_lens 需要从 (batch_size, n_query) / (batch_size,) 扩张为(batch_size*H, n_query) / (batch_size*H,)\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.H, dim=0)\n",
    "\n",
    "        output = self.attention(Q_, K_, V_, valid_lens) # (batch_size*H, n_query, w). w = num_hiddens//H\n",
    "\n",
    "        return self.W_o( multihead_transpose_o(output, self.H) ) # (batch_size, n_query, num_hiddens=H*w)\n",
    "\n",
    "num_heads, num_hiddens, dropout = 3, 9, 0.\n",
    "net = MultiHeadAttention(num_heads, num_hiddens, dropout)\n",
    "\n",
    "Q = torch.tensor([[3, 4, 1, 0],\n",
    "                    [2, 1, 0, 1],\n",
    "                    [1, 4, 5, 5]], dtype=torch.float32)\n",
    "Q = Q.unsqueeze(0).repeat(3, 1, 1)\n",
    "print(Q)\n",
    "\n",
    "y_hat = net(Q, Q, Q)\n",
    "print(y_hat)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
