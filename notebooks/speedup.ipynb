{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af2ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "import collections\n",
    "import multiprocessing\n",
    "import typing as t\n",
    "\n",
    "# Assuming these are defined elsewhere or passed in\n",
    "# For demonstration, let's provide dummy implementations\n",
    "def get_pair_counts(tokens: t.List[int], p_counts: t.Dict[tuple[int, int], int]):\n",
    "    \"\"\"\n",
    "    Counts pairs of adjacent tokens in a list and updates the given p_counts dictionary.\n",
    "    This function should be designed to update a shared dictionary safely (though here it returns updates).\n",
    "    For multiprocessing, it's safer for each process to return its own counts, then combine.\n",
    "    \"\"\"\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pair = (tokens[i], tokens[i+1])\n",
    "        p_counts[pair] = p_counts.get(pair, 0) + 1\n",
    "    return p_counts # In multiprocessing, each worker returns its own dict\n",
    "\n",
    "def merge_pair(tokens: t.List[int], pair_to_merge: tuple[int, int], new_token_id: int) -> t.List[int]:\n",
    "    \"\"\"\n",
    "    Merges occurrences of a specific pair of tokens into a new token ID.\n",
    "    \"\"\"\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == pair_to_merge:\n",
    "            merged_tokens.append(new_token_id)\n",
    "            i += 2 # Skip both tokens that were merged\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return merged_tokens\n",
    "\n",
    "# --- Your main class/function where the loop resides ---\n",
    "\n",
    "class BPEProcessor:\n",
    "    def __init__(self, num_merges: int, num_processes: int = None):\n",
    "        self._num_merges = num_merges\n",
    "        # Use CPU count if num_processes is not specified\n",
    "        self._num_processes = num_processes if num_processes is not None else multiprocessing.cpu_count()\n",
    "\n",
    "    def perform_merges(self, initial_chunks_tokens: t.List[t.List[int]], initial_vocab: t.Dict[int, bytes]):\n",
    "        \"\"\"\n",
    "        Performs BPE merges with multiprocessing acceleration.\n",
    "        \"\"\"\n",
    "        chunks_tokens = initial_chunks_tokens\n",
    "        vocab = initial_vocab\n",
    "        merge_ranks: t.Dict[tuple[int, int], int] = {} # Stores merge rules (pair -> new_token_id)\n",
    "\n",
    "        print(f\"Starting BPE merges with {self._num_processes} processes...\")\n",
    "\n",
    "        # Create a multiprocessing pool\n",
    "        # It's good practice to create the pool outside the loop if it's long-running\n",
    "        # and you want to reuse processes.\n",
    "        # However, for a fixed number of merges, creating/closing inside the loop might be simpler\n",
    "        # if the pool is not expected to be reused extensively across different high-level tasks.\n",
    "        # For this specific scenario (looping through _num_merges), let's create it once outside.\n",
    "        with multiprocessing.Pool(processes=self._num_processes) as pool:\n",
    "            for i in range(self._num_merges):\n",
    "                print(f\"\\n--- Merge Iteration {i+1}/{self._num_merges} ---\")\n",
    "\n",
    "                # --- Accelerate 1: Accumulate Pair Counts ---\n",
    "                # Each process will call get_pair_counts on a chunk of tokens and return its local counts.\n",
    "                # pool.map or pool.starmap are good for this.\n",
    "                # Here, we'll map a helper that wraps get_pair_counts.\n",
    "                \n",
    "                # Each process will receive one 'tokens' list from chunks_tokens\n",
    "                # and return its own dictionary of pair counts for that list.\n",
    "                all_partial_p_counts = pool.map(\n",
    "                    lambda tokens_list: get_pair_counts(tokens_list, {}), # Pass an empty dict for each process\n",
    "                    chunks_tokens\n",
    "                )\n",
    "                \n",
    "                # Manually accumulate all partial p_counts from workers\n",
    "                p_counts: t.Dict[tuple[int, int], int] = collections.defaultdict(int)\n",
    "                for partial_counts in all_partial_p_counts:\n",
    "                    for pair, count in partial_counts.items():\n",
    "                        p_counts[pair] += count\n",
    "\n",
    "                if not p_counts:\n",
    "                    print(\"No more pairs to merge. Stopping early.\")\n",
    "                    break\n",
    "\n",
    "                # From p_counts find occur-most pair of tokens (two IDs) as top_pair\n",
    "                occur_most_pair: tuple[int, int] = max(p_counts, key=p_counts.get)\n",
    "                new_token: int = i + 256 # Use merge rank as new token ID\n",
    "\n",
    "                merge_ranks[occur_most_pair] = new_token # Record merge: rank as new token\n",
    "                vocab[new_token] = vocab[occur_most_pair[0]] + vocab[occur_most_pair[1]] # Record new token corresponding bytes\n",
    "\n",
    "                print(f\"Merging pair {occur_most_pair} (representing '{vocab[occur_most_pair[0]].decode(errors='replace')}' + '{vocab[occur_most_pair[1]].decode(errors='replace')}') into new token ID {new_token}\")\n",
    "\n",
    "                # --- Accelerate 2: Update chunks_tokens ---\n",
    "                # Each process will call merge_pair on a chunk of tokens and return the updated chunk.\n",
    "                # Use functools.partial to fix the pair_to_merge and new_token_id arguments.\n",
    "                \n",
    "                # Create a partially applied function for merge_pair\n",
    "                from functools import partial\n",
    "                merge_func_for_pool = partial(merge_pair,\n",
    "                                              pair_to_merge=occur_most_pair,\n",
    "                                              new_token_id=new_token)\n",
    "\n",
    "                # Map this partial function over all token chunks\n",
    "                chunks_tokens = pool.map(merge_func_for_pool, chunks_tokens)\n",
    "\n",
    "        print(\"\\nBPE merges completed.\")\n",
    "        return chunks_tokens, merge_ranks, vocab\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Dummy initial data for demonstration\n",
    "    # Initial tokens are ASCII values (b'a' -> 97, b'b' -> 98, etc.)\n",
    "    # Let's say we start with some basic words\n",
    "    initial_tokens_data = [\n",
    "        [ord('a'), ord('b'), ord('a'), ord('b'), ord('a')], # ababa\n",
    "        [ord('a'), ord('b'), ord('c')],                     # abc\n",
    "        [ord('c'), ord('a'), ord('b')],                     # cab\n",
    "        [ord('a'), ord('b'), ord('a'), ord('c')]            # abac\n",
    "    ]\n",
    "\n",
    "    # Initial vocab for base bytes (0-255 ASCII)\n",
    "    initial_vocab_data = {i: bytes([i]) for i in range(256)}\n",
    "\n",
    "    # Create an instance of the processor\n",
    "    # For testing, you might use 2 processes, for production use multiprocessing.cpu_count()\n",
    "    processor = BPEProcessor(num_merges=3, num_processes=2) # Perform 3 merges with 2 processes\n",
    "\n",
    "    final_chunks, final_merge_ranks, final_vocab = processor.perform_merges(\n",
    "        initial_tokens_data,\n",
    "        initial_vocab_data\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Final Results ---\")\n",
    "    print(\"Final Chunks Tokens:\", final_chunks)\n",
    "    print(\"Final Merge Ranks:\", final_merge_ranks)\n",
    "    print(\"Final Vocab:\")\n",
    "    for token_id, token_bytes in final_vocab.items():\n",
    "        try:\n",
    "            print(f\"  {token_id}: '{token_bytes.decode('utf-8')}'\")\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"  {token_id}: {token_bytes} (undecodable)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokens = []\n",
    "    print(get_pair_counts(tokens))\n",
    "\n",
    "    p_counts = {}\n",
    "    returned = get_pair_counts(tokens, p_counts)\n",
    "\n",
    "    print(returned)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c30a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import typing as t\n",
    "import os\n",
    "import multiprocessing # 提前导入用于后续的多进程部分\n",
    "from functools import partial\n",
    "\n",
    "# 假设你的 get_pair_counts 和 merge_pair 函数已经定义好\n",
    "# 它们应该接受单个 tokens 列表作为输入\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 辅助函数 (保持不变或略作调整以适应单次调用)\n",
    "# ----------------------------------------------------\n",
    "def get_pair_counts_for_chunk(tokens: t.List[int]) -> t.Dict[tuple[int, int], int]:\n",
    "    \"\"\"\n",
    "    计算单个 token 块的 pair 计数，并返回本地字典。\n",
    "    \"\"\"\n",
    "    p_counts_local: t.Dict[tuple[int, int], int] = collections.defaultdict(int)\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pair = (tokens[i], tokens[i+1])\n",
    "        p_counts_local[pair] += 1\n",
    "    return p_counts_local\n",
    "\n",
    "def merge_pair_in_chunk(tokens: t.List[int], pair_to_merge: tuple[int, int], new_token_id: int) -> t.List[int]:\n",
    "    \"\"\"\n",
    "    在单个 token 块中执行合并操作。\n",
    "    \"\"\"\n",
    "    merged_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i + 1 < len(tokens) and (tokens[i], tokens[i+1]) == pair_to_merge:\n",
    "            merged_tokens.append(new_token_id)\n",
    "            i += 2\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return merged_tokens\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "class BPEMemoryOptimized:\n",
    "    def __init__(self, pat_str: str, num_merges: int):\n",
    "        self.pat_str = pat_str\n",
    "        self._num_merges = num_merges\n",
    "        self._vocab: t.Dict[int, bytes] = {i: bytes([i]) for i in range(256)}\n",
    "        self._merge_ranks: t.Dict[tuple[int, int], int] = {} # 存储合并规则 (pair -> new_token_id)\n",
    "        self._next_token_id = 256 # 从 256 开始分配新的 token ID\n",
    "\n",
    "    def _tokenize_and_apply_merges(self, text_chunk: str) -> t.List[int]:\n",
    "        \"\"\"\n",
    "        将文本块转换为 token ID 列表，并应用所有当前已存在的合并规则。\n",
    "        这个函数在每次从磁盘读取文本时被调用。\n",
    "        \"\"\"\n",
    "        # 1. 初始按 pat_str 分割并编码\n",
    "        str_chunks = re.findall(self.pat_str, text_chunk)\n",
    "        tokens_list = []\n",
    "        for s_chunk in str_chunks:\n",
    "            tokens_list.extend(list(s_chunk.encode('utf-8'))) # 转换为 int 列表\n",
    "\n",
    "        # 2. 应用所有已学到的合并规则\n",
    "        # 这里需要一个高效的方式来循环应用所有 merge_ranks\n",
    "        # 最简单但不一定最快的方式是不断迭代直到没有变化\n",
    "        # 更优的方法是构建一个 BPE decoder/encoder 结构，一次性编码\n",
    "        # 为了演示，我们使用一个简单的迭代合并过程\n",
    "        current_tokens = list(tokens_list) # 创建副本\n",
    "        for pair_to_merge, new_token_id in self._merge_ranks.items():\n",
    "            current_tokens = merge_pair_in_chunk(current_tokens, pair_to_merge, new_token_id)\n",
    "        return current_tokens\n",
    "\n",
    "    def train(self, corpus_filepath: str, batch_size_lines: int = 1000):\n",
    "        \"\"\"\n",
    "        进行 BPE 训练，优化了内存占用，通过分批处理语料库。\n",
    "        `corpus_filepath`: 大型文本文件的路径。\n",
    "        `batch_size_lines`: 每次从文件中读取并处理的行数。\n",
    "        \"\"\"\n",
    "        print(f\"开始 BPE 训练，共 {self._num_merges} 轮合并，批处理大小为 {batch_size_lines} 行。\")\n",
    "\n",
    "        # 使用 multiprocessing.Pool 来加速计算\n",
    "        with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool: # 使用所有可用核心\n",
    "            for i in range(self._num_merges):\n",
    "                p_counts: t.Dict[tuple[int, int], int] = collections.defaultdict(int)\n",
    "                print(f\"\\n--- 合并迭代 {i+1}/{self._num_merges} ---\")\n",
    "\n",
    "                # --- 内存优化和并行计算 p_counts ---\n",
    "                # 不再将整个 chunks_tokens 加载到内存。\n",
    "                # 而是逐批读取文件，对每个批次进行 tokenization 和 pair 计数。\n",
    "                # 并且在每次迭代中，都重新从原始语料库读取，并应用“目前为止”所有已学到的合并规则。\n",
    "                # 这样，`get_pair_counts_for_chunk` 就能在当前正确的 tokens 表示上工作。\n",
    "\n",
    "                batch_texts = []\n",
    "                current_batch_tokens_lists = [] # 存储当前批次的 tokens 列表\n",
    "\n",
    "                with open(corpus_filepath, 'r', encoding='utf-8') as f:\n",
    "                    for line_idx, line in enumerate(f):\n",
    "                        batch_texts.append(line)\n",
    "                        if (line_idx + 1) % batch_size_lines == 0:\n",
    "                            # 处理一个批次\n",
    "                            batch_corpus_text = \"\".join(batch_texts)\n",
    "\n",
    "                            # 将当前批次文本转换成 token ID 列表，并应用所有已学到的合并规则\n",
    "                            # 这一步是计算密集型，可以并行\n",
    "                            processed_tokens_list_for_batch = self._tokenize_and_apply_merges(batch_corpus_text)\n",
    "                            current_batch_tokens_lists.append(processed_tokens_list_for_batch)\n",
    "\n",
    "                            batch_texts = [] # 重置批次\n",
    "\n",
    "                    # 处理文件中剩余的行\n",
    "                    if batch_texts:\n",
    "                        batch_corpus_text = \"\".join(batch_texts)\n",
    "                        processed_tokens_list_for_batch = self._tokenize_and_apply_merges(batch_corpus_text)\n",
    "                        current_batch_tokens_lists.append(processed_tokens_list_for_batch)\n",
    "\n",
    "                # 将 current_batch_tokens_lists 分发给多个进程，计算 pair counts\n",
    "                # pool.map 会返回一个列表，其中包含每个进程计算的局部 p_counts\n",
    "                all_partial_p_counts = pool.map(get_pair_counts_for_chunk, current_batch_tokens_lists)\n",
    "\n",
    "                # 聚合所有局部 pair counts\n",
    "                for partial_counts in all_partial_p_counts:\n",
    "                    for pair, count in partial_counts.items():\n",
    "                        p_counts[pair] += count\n",
    "\n",
    "                if not p_counts:\n",
    "                    print(\"没有更多可合并的 pair，提前停止。\")\n",
    "                    break\n",
    "\n",
    "                # 找出出现次数最多的 pair\n",
    "                occur_most_pair: tuple[int, int] = max(p_counts, key=p_counts.get)\n",
    "                new_token_id: int = self._next_token_id\n",
    "                self._next_token_id += 1\n",
    "\n",
    "                # 记录新的合并规则和词汇\n",
    "                self._merge_ranks[occur_most_pair] = new_token_id\n",
    "                self._vocab[new_token_id] = self._vocab[occur_most_pair[0]] + self._vocab[occur_most_pair[1]]\n",
    "\n",
    "                print(f\"合并 pair {occur_most_pair} (新 token ID: {new_token_id})\")\n",
    "\n",
    "                # 注意：这里不再需要显式地更新一个巨大的 `chunks_tokens` 列表。\n",
    "                # 下一轮迭代时，`_tokenize_and_apply_merges` 会从头读取文件并应用最新的 `_merge_ranks`。\n",
    "\n",
    "        print(\"\\nBPE 训练完成。\")\n",
    "        return self._vocab, self._merge_ranks\n",
    "\n",
    "# --- 使用示例 (需要一个较大的虚拟语料库文件) ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建一个虚拟的大语料库文件 (例如 100MB)\n",
    "    dummy_corpus_path = \"large_dummy_corpus.txt\"\n",
    "    if not os.path.exists(dummy_corpus_path):\n",
    "        print(f\"创建虚拟语料库文件: {dummy_corpus_path}\")\n",
    "        with open(dummy_corpus_path, 'w', encoding='utf-8') as f:\n",
    "            for _ in range(50000): # 写入多行\n",
    "                f.write(\"The quick brown fox jumps over the lazy dog. \" * 20 + \" hello world \" * 10 + \"\\n\")\n",
    "        print(\"虚拟语料库已创建。\")\n",
    "\n",
    "    # BPE 分词器实例\n",
    "    # pat_str 定义了如何初步分割文本（例如，按单词、空格、标点）\n",
    "    bpe_trainer = BPEMemoryOptimized(pat_str=r\"\\w+|\\s+|[^\\w\\s]\", num_merges=20)\n",
    "    final_vocab, final_merges = bpe_trainer.train(dummy_corpus_path, batch_size_lines=1000)\n",
    "\n",
    "    print(\"\\n--- 训练结果 ---\")\n",
    "    print(f\"最终词汇表大小: {len(final_vocab)}\")\n",
    "    print(f\"合并规则数量: {len(final_merges)}\")\n",
    "    # 可以进一步打印部分词汇和规则进行检查\n",
    "    # print(dict(list(final_vocab.items())[256:266]))\n",
    "    # print(dict(list(final_merges.items())[:10]))\n",
    "\n",
    "    # 清理虚拟文件 (可选)\n",
    "    # os.remove(dummy_corpus_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
