{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37c9180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88683feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n",
      "1 GPU detected\n",
      "torch version as 1.13.1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    print(f\"{torch.cuda.device_count()} GPU detected\")\n",
    "else:\n",
    "    print(\"cuda not available\")\n",
    "\n",
    "print(f\"torch version as {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038c6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('../../data', 'bert/wikitext-2', 'wiki.train.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9509ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki(data_dir):\n",
    "    with open(data_dir, 'r') as f:\n",
    "        lines = f.readlines() # list of strings. each string is several sentences(joined by ' . '), that is a paragraph\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) > 2] # list of sentence lists\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c89f30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14222, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = _read_wiki(data_dir)\n",
    "len(paragraphs), len(paragraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f964ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    # paragraphs: num_paragraphs 个 paragraph, 每个paragraph是list of sentences(or list of tokens_lst)\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        next_sentence = random.choice(random.choice(paragraphs)) # 随机选一个paragraph, 再从该段随机选一句\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next # 输出1个sentence, next sentence, 是否连接flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8abb718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_segments(tokens_a, tokens_b=None):\n",
    "    # input token_a: list of tokens, [tk1, tk2,...,tkN]\n",
    "    # input token_b if not none: list of tokens, [tk1, tk2,...,tkN]\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    segments = [0] * len(tokens)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1]*(len(tokens_b) + 1)\n",
    "    # 拼接两个tokens list和<cls><sep>, 并输出它们的segments\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a9ed4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    # paragraph is a list of lists: each list is a list of tokens(a sentence)\n",
    "    # paragraphs is a list of paragraph\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph)-1): # 当前paragraph\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i+1], paragraphs)\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len: # 如果当前前后两句token数量+<cls>+2个<sep>数量超过max_len, 放弃当前两句\n",
    "            continue\n",
    "        tokens, segments = get_tokens_segments(tokens_a, tokens_b) # 拼接后的token list, 和对应的segments\n",
    "        nsp_data_from_paragraph.append( [tokens, segments, is_next] )\n",
    "    return nsp_data_from_paragraph # 输出一个list, 元素是nsp单样本: [拼接后token list, segment list, 是否相邻flag]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deb85918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对一个token list, 输入可mask的token positions(<cls>和<sep>不可被替换), 以及该token list要mask的token数量\n",
    "# 输出替换后的token list, 替换的<mask>在list中的positions, 以及真实label token list\n",
    "def _replace_mlm_tokens(tokens, candidate_mask_positions, num_mlm_masks, vocab):\n",
    "    mlm_input_tokens = [token for token in tokens] # 将输入token list拷贝下来\n",
    "    mask_positions_labels = [] # 记录(mask_position, mask_label) pair\n",
    "    random.shuffle(candidate_mask_positions) # 打乱\n",
    "    for mask_position in candidate_mask_positions:\n",
    "        if len(mask_positions_labels) >= num_mlm_masks: # 当已经作了足够次数mask操作, 退出循环\n",
    "            break\n",
    "        mask_token = None\n",
    "        if random.random() < 0.8: # 80%的概率, 用<mask>去mask\n",
    "            mask_token = '<mask>'\n",
    "        else:\n",
    "            if random.random() < 0.5: # 10%的概率, 用随机token去mask\n",
    "                mask_token = random.choice(vocab.idx_to_tokens)\n",
    "            else: # 10%的概率, 用自身去mask\n",
    "                mask_token = tokens[mask_position]\n",
    "        mlm_input_tokens[mask_position] = mask_token # mask操作\n",
    "        mask_positions_labels.append( (mask_position, tokens[mask_position]) ) # 记录被mask的token位置, 以及真实token\n",
    "    return mlm_input_tokens, mask_positions_labels # 输出token list, list of (position_idx, true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da231343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对一个token list, 计算可mask的token positions, 计算该token list要mask的token数量\n",
    "# 然后作mask操作\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_mask_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in ('<cls>', '<sep>'):\n",
    "            candidate_mask_positions.append(i)\n",
    "    num_mlm_masks = max(1, round(len(tokens)*0.15))\n",
    "    mlm_input_tokens, mask_positions_labels = _replace_mlm_tokens(tokens, candidate_mask_positions, num_mlm_masks, vocab)\n",
    "    mask_positions_labels = sorted(mask_positions_labels, key=lambda x: x[0]) # 将mask_positions_labels按照positions从小到大排列\n",
    "    mask_positions = [v[0] for v in mask_positions_labels]\n",
    "    mask_labels = [v[1] for v in mask_positions_labels]\n",
    "    return vocab[mlm_input_tokens], mask_positions, vocab[mask_labels] # 输出masked token_idx list, mask position list, mask true token_idx list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个任务的输出组合\n",
    "# [two_sentence_token_idx_list, mask_position_list, mask_label_token_idx_list], (two_sentence_segment_list, is_next_flag)\n",
    "# 为了batch化处理, 将 two_sentence_token_idx_list/mask_position_list/mask_label_token_idx_list/two_sentence_segment_list 作pad到统一长度\n",
    "# 根据mask_position_list/mask_label_token_idx_list, 同步生成一个mlm_weight_list, 对于pad元素权重设0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_masks = round(max_len*0.15) # token list的最大长度乘以0.15\n",
    "    all_tokens_idx, all_segments, valid_lens = [], [], []\n",
    "    all_mask_positions, all_mlm_weights, all_mlm_labels_idx = [], [], []\n",
    "    nsp_labels = []\n",
    "    for tokens_idx, mask_positions, mlm_labels_idx, segments, if_next in examples:\n",
    "        # pad tokens_idx\n",
    "        pad_tokens_idx = tokens_idx + ['<pad>']*(max_len - len(tokens_idx))\n",
    "        all_tokens_idx.append( torch.tensor(vocab[pad_tokens_idx], dtype=torch.int64) )\n",
    "        # pad segments\n",
    "        pad_segments = segments + [0]*(max_len - len(segments))\n",
    "        all_segments.append( torch.tensor(pad_segments, dtype=torch.int64) )\n",
    "        # valid_lens\n",
    "        valid_lens.append( torch.tensor(len(tokens_idx), dtype=torch.float32) )\n",
    "        # pad mask_positions\n",
    "        pad_mask_positions = mask_positions + [0]*(max_num_mlm_masks - len(mask_positions))\n",
    "        all_mask_positions.append( torch.tensor(pad_mask_positions, dtype=torch.int64) )\n",
    "        # pad mlm_labels_idx\n",
    "        pad_mlm_labels_idx = mlm_labels_idx + [0]*(max_num_mlm_masks - len(mlm_labels_idx))\n",
    "        all_mlm_labels_idx.append( torch.tensor(pad_mlm_labels_idx, dtype=torch.int64) )\n",
    "        # weights for mlm_labels_idx: 0 for pad\n",
    "        mlm_labels_weight = [1]*len(mlm_labels_idx) + [0]*(max_num_mlm_masks - len(mlm_labels_idx))\n",
    "        all_mlm_weights.append( torch.tensor(mlm_labels_weight, dtype=torch.float32) )\n",
    "        nsp_labels.append( torch.tensor(if_next, dtype=torch.int64) )\n",
    "    return all_tokens_idx, all_segments, valid_lens, all_mask_positions, all_mlm_weights, all_mlm_labels_idx, nsp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99140f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7fa0e5faa810>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[1,2,3] ,[4,5,6]]\n",
    "tuple(tensor[0] for tensor in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4518d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['a', 'b', 'c', 'a']\n",
    "x.remove('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20b54de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c', 'a']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
