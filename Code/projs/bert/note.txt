这个project是经典bert的pytorch版实现, 包括pretrain/fine-tune.
这个project可能包括经典bert的应用(pytorch版), 包括sentimental analysis/natural language inference.

Large-Scale Pretrain 
1. bert等大规模预训练模型出现的先决条件
    1. transformer的scalability, 即transformer能处理超大规模、多模态的训练数据
    2. 经典狭义的transfomer专注于翻译translation任务, encoder用来表征input sequence, 然后decoder用来生成target sequence.
       为了通用的下游任务, 预训练模型的真正用武之地.

2. bert是encoder-only的transformer variant: Bidirectional Encoder Representations from Transformers
    encoder-only的模型作用在表征: represent token sequence(N tokens) into tensor sequence(N tensors)

    bert将input sequence随机mask掉其中一个token, 然后在input sequence最前面添加表示位置的cls_token
    在前向过程的sel-attetion部分, 类似transformer的encoder, 是不加自回归限制的, 即每一个位置的token都参与表征了其他位置的tokens. 
    从而, mask位置的token可以从其前、后双向的tokens表征出来, 即bert名字中(Bidirectional)的意义.

3. fine-tune BERT 
    将pretrain bert model的token representions作为下游任务(scratch)的输入, 直至输出scratch任务的label, 组建成一个完整的模型.
    用scratch任务的数据(input和label)来train scratch layers, 以及update bert modules.
    有一种下游任务叫text pair classification, 两个sequence是否配对. 这是因为bert的预训练任务中, 还有一个loss, 是预测1个sentence是否
    直接连接另1个. 然而在一些bert变种模型中(RoBERTa), 研究者发现这个任务的loss是无用的. bert的变种很多, 有结构上的变化, 更有预训练任务的变化.

以往词向量化模型的局限性:
    context-independent   -----> context-sensitive
    word2vec模型(skipgram/cbow/glove/subword): 没有考虑语境(context-independent), word2vec将一个token映射到固定的vector, 不考虑语境
业界有人提出了ELMo模型, 它将input sequence作为输入, 给出序列中每一个词的embedding representation. 具体来说, ELMo使用预训练的BiLSTM, 然后将
token的所有中间层表示组合起来, 成为该token的输出表示. 在使用ELMo时, 将ELMo的输出表示作为额外特征, 加入到工作模型中. 在训练该工作模型的时候,
ELMo是不会被更新的, 也就是说ELMo的输出纯粹被当成了工作模型的特征.
    Task-Specific   -----> Task-Agnostic
    ELMo模型仅仅相当于是特征工程的算子, 为解决每个具体任务, 仍然需要搭建具体的工作模型(Task-Specific). 
业界提出了GPT模型(Generative Pre-Training). 它基于transformer的decoder, 可以fine-tune使用, 即以最小的变化续接输出层, 从头输入input, 输出任务label,
预训练模型一起被更新.
    Unidirectional   -----> Bidirectional


BERT正是一个context-sensitive/Task-Agnostic/Bidirectional的模型. 「语境相关/任务无关/双向上下文相关」

bert的输入:
bert有两个预训练任务: 完形填空 + 上下句是否连续判断.
    第一个任务的输入序列是[<cls>, <sA_tk_1>, ..., <sA_tk_K>, <sep>]
    第二个任务的输入序列是[<cls>, <sA_tk_1>, ..., <sA_tk_N>, <sep>, <sB_tk_1>,...,<sB_tk_M>, <sep>]
所以bert input sequence有两种可能性, 单条text sequence or 双条text sequence. 同时输出对应的segment labels:
    对第一种输入序列, segments = [0, 0,..., 0], K个0
    对第二种输入序列, segments = [0, 0,..., 0, 1, 1,..., 1], N个0 + M个1

token序列 embedding + segment labels embedding + position embedding, 三者之和作为bert encoder的输入

bert encoder层就是原transformer encoder的复用. 总共12个block, hidden size 768, head num 12
bert task层要完成两个预训练任务: 完形填空 + 上下句是否连续判断

完形填空: Masked Language Model(MLM)
    随机抽取15%的tokens, 选为mask token, 用各mask token同序列的其他tokens去预测这个mask token
    在一个token被选为mask token之后, 还依概率作如下选择:
    1. 80%的概率, 该mask token被替换为<mask>
    2. 10%的概率, 该mask token被替换为词汇表vocab中其他随机token
    3. 10%的概率, 该mask token不作替换, 原token保留
原因: MLM任务的根本目的, 是为了表征序列中的每一个token, 而不仅仅是mask token. 过多关注<mask>预测, 会让模型过多关注mask token.
而finetune时, 是不会有<mask>这一特殊token的. 在数据制作时加入这些noise工作, 让token表征时注重每一个token的表征.

MLM部分的输入:
    1. batch tokens embedding: (batch_size, seq_len, num_hiddens)
    2. pred_positions: (batch_size, num_positions)
即每一条input sequence, 有num_positions个mask tokens, 这些mask tokens的位置index就记录在pred_positions的一行
MLM部分的输出:
    用pred_positions位置的token embeddings, 拿去预测具体的token分布, 即(batch_size, num_positions, vocab_size)的输出
    与真正的label tokens(batch_size, num_positions)作对比, 求出交叉熵损失

上下句判断: Next Sentence Prediction(NSP)
    每一行样本取上下句, 50%的概率上下句在原文中相连, 50%的概率上下句在原文中不相连
    在样本句首添加代表位置的cls_token, 在encoder前向过程中, 由于cls_token与所有的tokens都经过了self-att运算, 所以它可以认为是「句向量」(两个sentences的综合)
    用cls_token的embdding预测, 这一行样本是否代表相邻的两个sentences

NSP部分的输入:
    1. 每一个text pair的第一个位置cls_token的embedding: (batch_size, num_hiddens)
NSP部分的输出
    binary分类的logits(batch_size, 2), 与真正的是否上下句标签(batch_size, )作对比, 求出交叉熵损失

全局loss采用: MLM部分的「当前样本平均一个mask位置预测的loss」+「当前样本上下句是否相连的二分类loss」作为当前样本的loss


考虑在bert model是为了fine-tune使用的, 所以在forward的时候, 要考虑fine-tune的使用模式. fine-tune的时候可能只输入一句, 且没有mask_positions作为输入