这个project是经典bert的pytorch版实现, 包括pretrain/fine-tune.
这个project可能包括经典bert的应用(pytorch版), 包括sentimental analysis/natural language inference.

Large-Scale Pretrain 
1. bert等大规模预训练模型出现的先决条件
    1. transformer的scalability, 即transformer能处理超大规模、多模态的训练数据
    2. 经典狭义的transfomer专注于翻译translation任务, encoder用来表征input sequence, 然后decoder用来生成target sequence.
       为了通用的下游任务, 预训练模型的真正用武之地.

2. bert是encoder-only的transformer variant: Bidirectional Encoder Representations from Transformers
    encoder-only的模型作用在表征: represent token sequence(N tokens) into tensor sequence(N tensors)

    bert将input sequence随机mask掉其中一个token, 然后在input sequence最前面添加表示位置的cls_token
    在前向过程的sel-attetion部分, 类似transformer的encoder, 是不加自回归限制的, 即每一个位置的token都参与表征了其他位置的tokens. 
    从而, mask位置的token可以从其前、后双向的tokens表征出来, 即bert名字中(Bidirectional)的意义.

3. fine-tune BERT 
    将pretrain bert model的token representions作为下游任务(scratch)的输入, 直至输出scratch任务的label, 组建成一个完整的模型.
    用scratch任务的数据(input和label)来train scratch layers, 以及update bert modules.
    有一种下游任务叫text pair classification, 两个sequence是否配对. 这是因为bert的预训练任务中, 还有一个loss, 是预测1个sentence是否
    直接连接另1个. 然而在一些bert变种模型中(RoBERTa), 研究者发现这个任务的loss是无用的. bert的变种很多, 有结构上的变化, 更有预训练任务的变化.

以往词向量化模型的局限性:
    context-independent   -----> context-sensitive
    word2vec模型(skipgram/cbow/glove/subword): 没有考虑语境(context-independent), word2vec将一个token映射到固定的vector, 不考虑语境
业界有人提出了ELMo模型, 它将input sequence作为输入, 给出序列中每一个词的embedding representation. 具体来说, ELMo使用预训练的BiLSTM, 然后将
token的所有中间层表示组合起来, 成为该token的输出表示. 在使用ELMo时, 将ELMo的输出表示作为额外特征, 加入到工作模型中. 在训练该工作模型的时候,
ELMo是不会被更新的, 也就是说ELMo的输出纯粹被当成了工作模型的特征.
    Task-Specific   -----> Task-Agnostic
    ELMo模型仅仅相当于是特征工程的算子, 为解决每个具体任务, 仍然需要搭建具体的工作模型(Task-Specific). 
业界提出了GPT模型(Generative Pre-Training). 它基于transformer的decoder, 可以fine-tune使用, 即以最小的变化续接输出层, 从头输入input, 输出任务label,
预训练模型一起被更新.
    Unidirectional   -----> Bidirectional


BERT正是一个context-sensitive/Task-Agnostic/Bidirectional的模型. 「语境相关/任务无关/双向上下文相关」