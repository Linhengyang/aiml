这个project是word Embedding(word2vec)各模型的pytorch版实现, 包括skip-gram/CBOW/Glove/SubwordEmbed, 训练方式包括负采样和层次softmax

skip-gram模型
每个word有两种身份, center word和context word, 从而每个word有两种factor表示: vector as context & vector as center

1. 在一个sentence(word sequence)中, context word在给定center word的条件下, 条件概率是可建模(似然函数)的
context word就是相对center word, 距离在预设定的距离m以内(即一个context window内)

P(Word_o | Word_c) =
        f(Word_o vector as context, Word_c vector as center) 分子
    sum{ f(all Word vector of vocab as context, Word_c vector as center) } 分母

2. 整个sentence从而可建模, 即: word sequence中每一个word轮流当center word, 考虑一个context window(左右各m个), 
context window内部的word视作context word, 它们的条件概率乘积, 视作该center word的context window的建模(似然函数)
所有word轮流当center word, 它们的context window的建模乘积, 视作该sentence的建模(似然函数)
 
通过最大化语料库中的所有sentence的建模(似然函数), 即可更新每个word的vector as context & vector as center
等价于: 最小化 - log( Prob( <context_word_vector, center_word_vector> ) ).
里面的 Prob( <context_word_vector, center_word_vector> ) 是softmax操作得到, logits是all_context_word_vectors和center_word_vector的内积



CBOW(continuous bag of words)模型
每个word有两种身份, center word和context word, 从而每个word有两种factor表示: vector as context & vector as center

1. 在一个sentence(word sequence)中, center word在给定context words的条件下, 条件概率是可建模(似然函数)的
context word就是相对center word, 距离在预设定的距离m以内(即一个context window内)

P(Word_c | Word_o1, Word_o2, ..., Word_o2m 共2m-1个context word) = 
        f(Word_c vector as center, Word_o1到Word_o2m所有2m-1个context words的vectors as context average pool作为context vector) 分子
    sum{ f(all Word vecotr of vocab as center, Word_o1到Word_o2m所有2m-1个context words的vectors as context average pool作为context vector) }分母

2. 整个sentence从而可建模, 即: word sequence中每一个word轮流当center word, 考虑一个context window(左右各m个), 
context window内部的word视作context word, 计算该center word在context window的条件概率, 视作该center word的建模(似然函数)
所有word轮流当center word, 它们的建模乘积, 视作该sentence的建模(似然函数)

通过最大化语料库中的所有sentence的建模(似然函数), 即可更新每个word的vector as context & vector as center
等价于: 最小化 - log( Prob( <center_word_vector, context_words_pool> ) ).
里面的 Prob( <center_word_vector, context_words_pool> ) 是softmax操作得到, logits是all_center_word_vectors和context_words_pool的内积



训练结束之后, 无论是skip-gram还是CBOW, 每个word都出现了两种身份下的embedding表示. 那么选哪个作为word的表示呢?
答: 选择word在建模时,作为条件的表示, 即在skip-gram模型中, 选择其center word身份下的vector as embeding; 在CBOW模型中, 选择其context word身份下的vector as embeddibg



近似训练 approximate trainig
可以看到, 如果从神经网络预测的角度出发看skip-gram和CBOW, 它们本质都是一个多分类模型, 分类维度V=vocab_size, 只不过分类logits的生成极其简单:
对skip-gram, logit = <context_word_vector, center_word_vector>, 对CBOW, logit = <center_word_vector, context_words_pool>
那么多分类模型都涉及softmax操作, 一个分类维度为vocab_size的softmax操作

两种近似训练的思想可以节省softmax运算开销
1. negative sampling
negative sampling的本质是「化多分类为二分类」. 即:
    1. 原样本的label变特征, 同时对该行数据创建一个新label=1, 意思是(原样本feature, 原样本label)这件事情发生了, y=1
    2. 那么y=0的新样本很显然就是指没有发生的(原样本feature, 非原样本label)对. 这里可以用随机生成的方式, 将K个非原样本labels和原样本feature配对, 生成K个y=0的负样本
    3. 用sigmoid去设计二分类的pred和loss

2. hierarchical softmax
hierarchical softmax的本质是「化多分类为N次二分类」. 即:
    1. 将所有vocab words用二叉树的叶子节点编码之后, 一次寻找目标的多分类可以视作N次二分类, 即在二叉树上从根节点开始的N次二分查找, 直到找到在叶子节点上的目标
    2. 给定条件vector(center word vector/context word pool), 那么
        Prob(目标vector, 条件vector) := 一些和根节点有关的概率的连乘, 这些和根节点有关的概率分别是:
            Prob(根节点vector, 条件vector) if 在这个根节点向左查找 else 1 - Prob(根节点vector, 条件vector)
    3. 上述所有的Prob(vector1, vector2)都用sigmoid(<vector1, vector2>)构拟

可以看出, 在hierarchical softmax中, 我们放弃了对目标vector(context word vector for skip-gram/center word vector for CBOW)的表示, 转而去表示根节点.
根节点数量 = 叶子节点数量 - 1, 所以不会造成存储空间的增长; 对于同一个条件vector, 用层次softmax找到所有目标目标(叶子节点)的概率之和还是等于1, 所以层次softmax生成的还是一个有效的多分类概率分布