一. 协同过滤 Collaborative Filtering
这个部分是协同过滤推荐「collaborative filtering recommender(cfrec)」算法的 pytorch版实现, 包括 matrix factorization/AutoRec/NeuralMF/SeqAwareRecsys
目的是 「基于用户行为的推荐」, 行为即"interaction", based on explicit显性/implicit隐形 interactions. 显性action有打分、点赞、不喜欢等, 隐性action有点击、曝光不点击等

协同过滤collaborative filtering的本质, 是为了构建「user-item interaction matrix」, 即用户-物品交互矩阵.
矩阵的元素可以是显性行为, 比如打分/是否点赞/是否喜欢等;也可以是隐性行为, 比如是否点击/是否浏览等.

对于显性explicit interactions, 一般认为: 数值本身有意义, 并且0代表Null而不是0分
interaction matrix中 0代表「完全未知」元, 即「i,j」元是0, 代表用户i对物品j的显性行为未知, 用户i还没有碰到物品j. 目标是预测这些未知元.
一般用rmse去构造「值」的损失
1. Matrix Factorizationn
本质是用户id特征, embedd为k维的latent vector; 物品id特征, embedd为k维的latent vector; 最后用两个latent vector的内积作为interaction的预测

2. AutoRec(ItemBased or UserBased)
本质是一个自编码器(完成恒等映射的MLP). 用已知的interaction元素作为label, 更新MLP中的参数.
最后将原已有的interaction矩阵作为输入, 输出同形状矩阵, 其在原interaction矩阵为0的位置的填充, 即是预测
简而言之: 完成已知interaction元素到自身的恒等映射MLP, 那么这个MLP作用在0元素上, 预计得到该位置的score(感觉不是很有道理).

3. NeuralMF




对于隐性implicit interactions, 一般认为: 数值本身无意义, 但数值之间的序有意义, 并且0代表0而不是Null
interaction matrix中 0代表「不足以做出行为」元, 即「i,j」元是0, 代表用户i对物品j没有做出行为.
用相对距离去构造「序」的损失







特征交叉
比如prov_id和age_id, value_size分别为prov_size和age_size. 那么一般是:
1 分别onehot成prov_size维和age_size维的onehot vectors, 
2 特征交叉, 笛卡尔乘积出一个 prov_size*age_size 维度的onehot vector, 可以看作是新特征 prov-age_id 的onehot表示,


onehot和embedding的数据预处理
在onehot和embed使用前, 都要对categorical data作 level-index 映射,
比如在vocab中, 对tokens按使用频次高低映射到0至vocab_size-1的index值;在user_id中, 对原user_id减1, 映射到0至u_size-1的index值

理解one-hot: nn.functional.one_hot(level_index, num_levels)
level-index值 ----> length为 num_levels的vector, 其中level-index值的位置填1, 其余位置都是0

理解embed:
单个categorical特征, 以user_id即users_tensor为例:
    users_tensor: (batch_size, ) ---embed---> (batch_size, k)
    理解方式1: 查表, 即 (user_size, k)的全表, indexing users_tensor里的index值, 得到（batch_size, k）的子表
    理解方式2: 线性映射, 即 users_tensor(batch_size,) ---onehot---> (batch_size, user_size) ---linear transfer W---> (batch_size, k)
    其中方式2的线性映射表W, 就是方式1中的全表

即:
「大类别中的类别单值」--->「低维dense向量」, 中间如果经过「高维onehot向量」,即为理解方式2
    「大类别中的类别单值」 --embed--> 「低维dense向量」
     scalar_value      --------->  [d1, d2, ..dk] dense vector

    「大类别中的类别单值」--onehot--> 「高维onehot 0-1向量」             -----linear----> 「低维dense向量」
     scalar_value     ---------->  [0, 0, ..,1,...0] onehot vector  -------------->  [d1, d2, ..dk] dense vector


多个categorical特征, 以user_id/item_id/device_id/context_id为例:
    (batch_size, 4(分别是uid, itemid, deviceid, contextid))
    与token embedding不同, token index是同一个特征, one-hot后特征维度是唯一的, 即vocab_size. 而多个categorical特征, 各自one-hot后特征维度是不同的.
    单个sample: (uid, iid, did, cid), one-hot后维度分别是(u_size, i_size, d_size, c_size)
    可以分别embed成 Eu, Ei, Ed, Ec 维度的dense vector, 那么该样本由length=4的1-D categorical vector变成length=Eu+Ei+Ed+Ec的1-D dense vector
    那么这样需要4个embedding, 分别是nn.embedding(u_size, Eu), nn.embedding(i_size, Ei), nn.embedding(d_size, Ed), nn.embedding(c_size, Ec), 然后拼接起来,
    最后的结果shape是: (batch_size, Eu+Ei+Ed+Ec)

    也可以统一embed成k维度的dense vector, 那么该样本由length=4的1-D level-index vector变成shape=(4, k)的2-D dense tensor, 可以继续摊平变成length=4*k的1-D dense vector
    这样只需要一个embedding, 是nn.embedding(u_size+i_size+d_size+c_size, k).
    理解方式: 1-D level-index vector [0001, 001, 01, 1] --onehot_concat--> [0,1,0,...,0,  0,1,0,...,0,  0,1,0,..,0,  0,1]这样一个sparse tensor(只在各特征相应位置是1)
    --linear transfer W或查表--> shape为(4, k)的dense tensor --flatten--> length为(4*k, )的dense vector 
    (这样理解下来, 其实就是让Eu=Ei=Ed=Ec=k, 分别embed后concat.)
    onehot_concat不能由nn.functional.one_hot函数实现, 因为这个函数只能实现单个categorical特征的onehot变换.参见Code.Utils.Common.onehot_for_multi_labels

总结:
1. 有一张统一的embedding权重表weights.
    对于「单个categorical特征」, 这张weights表的统一性是天然的, 形状是(value_size, k)
    对于「多个不同categorical特征」, weights表的统一性并不天然. 什么叫统一? 对每个特征的每个value值, 有唯一的k维向量值对应. 
    幸运的是, 经过offset操作, 这张统一的weights表也可以得到, 形状是(value_size_feat1+...+value_size_featN, k)
2. onehot操作(并concat) + onehot向量乘以权重表weights操作 ---> embedd操作
    对于「单个categorical特征」, onehot操作后, 得到shape为(value_size, )的独热行向量, 
    然后右乘weights表, 得到embeddings结果, shape为(k, )
    
    对于「N个同一categorical特征」, onehot操作后, 得到shape为(N, value_size)的独热行向量们, 
    然后右乘weights表, 得到embeddings结果, shape为(N, k)

    对于「M个不同categorical特征」, 各自onehot操作并concat后, 得到shape为(value_size_feat1+...+value_size_featM, )的稀疏行向量,
    然后右乘weights表, 得到embeddings结果, shape为(M, k). 考虑属于同一样本, 可以选择摊平为(M*k, )的结果

由此可见, 只有在onehot操作后, embedding操作和右乘weights矩阵是一回事, 因为独热行向量x 右乘 embedding weights等于embdding结果.